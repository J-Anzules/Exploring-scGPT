12:11:41-scGPT-INFO-<module>: match 2808/3000 genes in vocabulary of size 60697.
12:11:41-scGPT-INFO-<module>: Resume model from ../Models/scGPT_human/best_model.pt, the model args will override the config ../Models/scGPT_human/args.json.
12:12:09-scGPT-INFO-__call__: Normalizing total counts ...
12:12:09-scGPT-INFO-__call__: Binning data ...
12:12:11-scGPT-INFO-__call__: Normalizing total counts ...
12:12:11-scGPT-INFO-__call__: Binning data ...
12:12:54-scGPT-INFO-<module>: train set number of samples: 7059, 
	 feature length: 1309
12:12:54-scGPT-INFO-<module>: valid set number of samples: 785, 
	 feature length: 1338
12:13:54-scGPT-INFO-<module>: Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
12:13:54-scGPT-INFO-<module>: Loading params encoder.enc_norm.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params encoder.enc_norm.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
12:13:55-scGPT-INFO-<module>: Loading params value_encoder.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params value_encoder.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params value_encoder.norm.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params value_encoder.norm.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params decoder.fc.0.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params decoder.fc.0.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params decoder.fc.2.weight with shape torch.Size([512, 512])
12:13:55-scGPT-INFO-<module>: Loading params decoder.fc.2.bias with shape torch.Size([512])
12:13:55-scGPT-INFO-<module>: Loading params decoder.fc.4.weight with shape torch.Size([1, 512])
12:13:55-scGPT-INFO-<module>: Loading params decoder.fc.4.bias with shape torch.Size([1])
12:13:55-scGPT-INFO-<module>: Total Pre freeze Params 51340819
12:13:55-scGPT-INFO-<module>: Total Post freeze Params 51340819
12:15:50-scGPT-INFO-train: | epoch   1 | 100/221 batches | lr 0.0001 | ms/batch 208.33 | loss  1.08 | cls  1.08 | err  0.33 | 
12:16:09-scGPT-INFO-train: | epoch   1 | 200/221 batches | lr 0.0001 | ms/batch 189.91 | loss  0.51 | cls  0.51 | err  0.16 | 
12:16:15-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:16:15-scGPT-INFO-<module>: | end of epoch   1 | time: 45.31s | valid loss/mse 0.4385 | err 0.1465
12:16:15-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:16:15-scGPT-INFO-<module>: Best model with score 0.4385
12:16:34-scGPT-INFO-train: | epoch   2 | 100/221 batches | lr 0.0001 | ms/batch 188.23 | loss  0.42 | cls  0.42 | err  0.13 | 
12:16:52-scGPT-INFO-train: | epoch   2 | 200/221 batches | lr 0.0001 | ms/batch 182.85 | loss  0.36 | cls  0.36 | err  0.12 | 
12:16:57-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:16:57-scGPT-INFO-<module>: | end of epoch   2 | time: 42.54s | valid loss/mse 0.5276 | err 0.1656
12:16:57-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:17:16-scGPT-INFO-train: | epoch   3 | 100/221 batches | lr 0.0001 | ms/batch 187.96 | loss  0.32 | cls  0.32 | err  0.10 | 
12:17:35-scGPT-INFO-train: | epoch   3 | 200/221 batches | lr 0.0001 | ms/batch 183.42 | loss  0.28 | cls  0.28 | err  0.09 | 
12:17:40-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:17:40-scGPT-INFO-<module>: | end of epoch   3 | time: 42.82s | valid loss/mse 0.3667 | err 0.1108
12:17:40-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:17:40-scGPT-INFO-<module>: Best model with score 0.3667
12:17:59-scGPT-INFO-train: | epoch   4 | 100/221 batches | lr 0.0001 | ms/batch 190.82 | loss  0.25 | cls  0.25 | err  0.08 | 
12:18:18-scGPT-INFO-train: | epoch   4 | 200/221 batches | lr 0.0001 | ms/batch 183.80 | loss  0.21 | cls  0.21 | err  0.07 | 
12:18:23-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:18:23-scGPT-INFO-<module>: | end of epoch   4 | time: 42.98s | valid loss/mse 0.5529 | err 0.1376
12:18:23-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:18:42-scGPT-INFO-train: | epoch   5 | 100/221 batches | lr 0.0001 | ms/batch 190.66 | loss  0.23 | cls  0.23 | err  0.08 | 
12:19:01-scGPT-INFO-train: | epoch   5 | 200/221 batches | lr 0.0001 | ms/batch 184.75 | loss  0.17 | cls  0.17 | err  0.06 | 
12:19:06-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:19:06-scGPT-INFO-<module>: | end of epoch   5 | time: 43.07s | valid loss/mse 0.4542 | err 0.1210
12:19:06-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:19:25-scGPT-INFO-train: | epoch   6 | 100/221 batches | lr 0.0001 | ms/batch 187.85 | loss  0.17 | cls  0.17 | err  0.05 | 
12:19:43-scGPT-INFO-train: | epoch   6 | 200/221 batches | lr 0.0001 | ms/batch 182.66 | loss  0.16 | cls  0.16 | err  0.05 | 
12:19:49-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:19:49-scGPT-INFO-<module>: | end of epoch   6 | time: 42.54s | valid loss/mse 0.4683 | err 0.1121
12:19:49-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:20:08-scGPT-INFO-train: | epoch   7 | 100/221 batches | lr 0.0001 | ms/batch 195.70 | loss  0.16 | cls  0.16 | err  0.05 | 
12:20:27-scGPT-INFO-train: | epoch   7 | 200/221 batches | lr 0.0001 | ms/batch 187.55 | loss  0.14 | cls  0.14 | err  0.05 | 
12:20:33-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:20:33-scGPT-INFO-<module>: | end of epoch   7 | time: 43.90s | valid loss/mse 0.5871 | err 0.1223
12:20:33-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:20:52-scGPT-INFO-train: | epoch   8 | 100/221 batches | lr 0.0000 | ms/batch 190.21 | loss  0.11 | cls  0.11 | err  0.03 | 
12:21:10-scGPT-INFO-train: | epoch   8 | 200/221 batches | lr 0.0000 | ms/batch 186.06 | loss  0.08 | cls  0.08 | err  0.03 | 
12:21:16-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:21:16-scGPT-INFO-<module>: | end of epoch   8 | time: 43.09s | valid loss/mse 0.7632 | err 0.1427
12:21:16-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:21:35-scGPT-INFO-train: | epoch   9 | 100/221 batches | lr 0.0000 | ms/batch 188.13 | loss  0.11 | cls  0.11 | err  0.03 | 
12:21:53-scGPT-INFO-train: | epoch   9 | 200/221 batches | lr 0.0000 | ms/batch 184.70 | loss  0.07 | cls  0.07 | err  0.02 | 
12:21:58-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:21:58-scGPT-INFO-<module>: | end of epoch   9 | time: 42.83s | valid loss/mse 0.5792 | err 0.1070
12:21:58-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:22:18-scGPT-INFO-train: | epoch  10 | 100/221 batches | lr 0.0000 | ms/batch 190.61 | loss  0.09 | cls  0.09 | err  0.03 | 
12:22:36-scGPT-INFO-train: | epoch  10 | 200/221 batches | lr 0.0000 | ms/batch 184.63 | loss  0.06 | cls  0.06 | err  0.02 | 
12:22:42-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:22:42-scGPT-INFO-<module>: | end of epoch  10 | time: 43.43s | valid loss/mse 0.6161 | err 0.1134
12:22:42-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
12:25:06-scGPT-INFO-test: Accuracy: 0.888, Precision: 0.800, Recall: 0.785, Macro F1: 0.782
12:28:10-scGPT-INFO-test: Accuracy: 0.888, Precision: 0.800, Recall: 0.785, Macro F1: 0.782
